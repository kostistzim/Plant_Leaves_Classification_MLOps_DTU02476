{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T14:52:38.028308Z",
     "start_time": "2025-01-22T14:52:37.271407Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T14:56:38.677496Z",
     "start_time": "2025-01-22T14:56:35.145868Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8h/82c89t014nv4j0sw212kn4640000gn/T/ipykernel_40479/2565670260.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"../models/model.pth\"))\n"
     ]
    }
   ],
   "source": [
    "from plant_leaves.model import PlantClassifier\n",
    "\n",
    "model = PlantClassifier().to(\"mps:0\")\n",
    "model.load_state_dict(torch.load(\"../models/model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "onnx_model = torch.onnx.export(\n",
    "    model,  # model being run\n",
    "    (torch.randn(1, 3, 240, 240).to(\"mps\"),),  # model input (or a tuple for multiple inputs)\n",
    "    \"plant_leaves_graph.onnx\",  # where to save the model (can be a file or file-like object)\n",
    "    input_names=[\"input\"],  # the model's input names\n",
    "    output_names=[\"output\"],\n",
    ")  # the model's output names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T14:46:22.574985Z",
     "start_time": "2025-01-22T14:46:22.572601Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(x: torch.Tensor) -> Any\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "print(inspect.signature(model.forward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4f/7kcbgj992s5fhk7wcryyv4hc0000gn/T/ipykernel_11656/1032847024.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pt = torch.load('../data/processed/test/targets.pt')\n"
     ]
    }
   ],
   "source": [
    "pt = torch.load(\"../data/processed/test/targets.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/tzikos/Desktop/DTU/MLOps/Project/Plant_Leaves_Classification_MLOps_DTU02476'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = \"~\"\n",
    "os.path.dirname(os.path.dirname(os.path.abspath(path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4f/7kcbgj992s5fhk7wcryyv4hc0000gn/T/ipykernel_47195/1688312756.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  targets = torch.load(\"../data/processed/train/targets.pt\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "targets = torch.load(\"../data/processed/train/targets.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "onnx_model = onnx.load(\"../models/model.onnx\")\n",
    "\n",
    "\n",
    "def prune_weights_randomly(onnx_model, prune_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Prunes weights in the ONNX model randomly.\n",
    "\n",
    "    Parameters:\n",
    "    - onnx_model: Loaded ONNX model.\n",
    "    - prune_ratio: Fraction of weights to set to zero (0 to 1).\n",
    "\n",
    "    Returns:\n",
    "    - pruned_model: Modified ONNX model.\n",
    "    \"\"\"\n",
    "    for initializer in onnx_model.graph.initializer:\n",
    "        # Get weights as a numpy array\n",
    "        weights = np.frombuffer(initializer.raw_data, dtype=np.float32)\n",
    "\n",
    "        # Randomly zero out a fraction of weights\n",
    "        mask = np.random.rand(*weights.shape) > prune_ratio\n",
    "        pruned_weights = weights * mask\n",
    "\n",
    "        # Update the initializer with pruned weights\n",
    "        initializer.raw_data = pruned_weights.tobytes()\n",
    "\n",
    "    return onnx_model\n",
    "\n",
    "\n",
    "# Prune the model\n",
    "pruned_model = prune_weights_randomly(onnx_model, prune_ratio=0.2)\n",
    "\n",
    "# Save the pruned model\n",
    "pruned_model_path = \"pruned_model.onnx\"\n",
    "onnx.save(pruned_model, pruned_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference output: [array([[0., 1.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Load the pruned model\n",
    "session = ort.InferenceSession(pruned_model_path)\n",
    "\n",
    "# Prepare input data (example)\n",
    "input_name = session.get_inputs()[0].name\n",
    "input_shape = session.get_inputs()[0].shape\n",
    "input_data = np.random.rand(*input_shape).astype(np.float32)\n",
    "\n",
    "# Run inference\n",
    "output_name = session.get_outputs()[0].name\n",
    "outputs = session.run([output_name], {input_name: input_data})\n",
    "\n",
    "print(\"Inference output:\", outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime_tools import optimizer\n",
    "\n",
    "optimized_model = optimizer.optimize_model(pruned_model_path)\n",
    "optimized_model.save_model_to_file(\"optimized_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_inference_time(model_path, input_shape, num_runs=100):\n",
    "    \"\"\"\n",
    "    Measures the average inference time of an ONNX model.\n",
    "\n",
    "    Parameters:\n",
    "    - model_path: Path to the ONNX model.\n",
    "    - input_shape: Shape of the input tensor.\n",
    "    - num_runs: Number of inference runs to average.\n",
    "\n",
    "    Returns:\n",
    "    - avg_time: Average inference time in milliseconds.\n",
    "    \"\"\"\n",
    "    session = ort.InferenceSession(model_path)\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    input_data = np.random.rand(*input_shape).astype(np.float32)\n",
    "\n",
    "    # Warm-up runs\n",
    "    for _ in range(10000):\n",
    "        session.run(None, {input_name: input_data})\n",
    "\n",
    "    # Measure inference time\n",
    "    start_time = time.time()\n",
    "    for _ in range(num_runs):\n",
    "        session.run(None, {input_name: input_data})\n",
    "    end_time = time.time()\n",
    "\n",
    "    avg_time = (end_time - start_time) / num_runs * 1000  # Convert to milliseconds\n",
    "    return avg_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model Inference Time: 30.12 ms\n",
      "Pruned Model Inference Time: 30.50 ms\n",
      "Optimized Pruned Model Inference Time: 30.61 ms\n"
     ]
    }
   ],
   "source": [
    "# Get input shape from the original model\n",
    "original_model_path = \"../models/model.onnx\"\n",
    "optimized_pruned_model_path = \"optimized_model.onnx\"\n",
    "session = ort.InferenceSession(original_model_path)\n",
    "input_shape = session.get_inputs()[0].shape\n",
    "\n",
    "# Measure inference times\n",
    "original_time = measure_inference_time(original_model_path, input_shape)\n",
    "pruned_time = measure_inference_time(pruned_model_path, input_shape)\n",
    "optimized_pruned_time = measure_inference_time(optimized_pruned_model_path, input_shape)\n",
    "\n",
    "# Display results\n",
    "print(f\"Original Model Inference Time: {original_time:.2f} ms\")\n",
    "print(f\"Pruned Model Inference Time: {pruned_time:.2f} ms\")\n",
    "print(f\"Optimized Pruned Model Inference Time: {optimized_pruned_time:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight Differences: {'backbone.blocks.0.0.se.conv_reduce.weight': np.float32(0.010197334), 'backbone.blocks.0.0.se.conv_reduce.bias': np.float32(0.27972263), 'backbone.blocks.0.0.se.conv_expand.weight': np.float32(0.009742614), 'backbone.blocks.0.0.se.conv_expand.bias': np.float32(0.31252247), 'backbone.blocks.0.1.se.conv_reduce.weight': np.float32(0.0097963065), 'backbone.blocks.0.1.se.conv_reduce.bias': np.float32(0.90575165), 'backbone.blocks.0.1.se.conv_expand.weight': np.float32(0.008433992), 'backbone.blocks.0.1.se.conv_expand.bias': np.float32(0.6111971), 'backbone.blocks.1.0.se.conv_reduce.weight': np.float32(0.0069128494), 'backbone.blocks.1.0.se.conv_reduce.bias': np.float32(2.034321), 'backbone.blocks.1.0.se.conv_expand.weight': np.float32(0.0066853017), 'backbone.blocks.1.0.se.conv_expand.bias': np.float32(0.41265675), 'backbone.blocks.1.1.se.conv_reduce.weight': np.float32(0.010195589), 'backbone.blocks.1.1.se.conv_reduce.bias': np.float32(0.054137204), 'backbone.blocks.1.1.se.conv_expand.weight': np.float32(0.008004337), 'backbone.blocks.1.1.se.conv_expand.bias': np.float32(0.31624782), 'backbone.blocks.1.2.se.conv_reduce.weight': np.float32(0.009861173), 'backbone.blocks.1.2.se.conv_reduce.bias': np.float32(0.36404824), 'backbone.blocks.1.2.se.conv_expand.weight': np.float32(0.0067649493), 'backbone.blocks.1.2.se.conv_expand.bias': np.float32(0.30408654), 'backbone.blocks.2.0.se.conv_reduce.weight': np.float32(0.007017374), 'backbone.blocks.2.0.se.conv_reduce.bias': np.float32(0.059536923), 'backbone.blocks.2.0.se.conv_expand.weight': np.float32(0.0067419666), 'backbone.blocks.2.0.se.conv_expand.bias': np.float32(0.39130723), 'backbone.blocks.2.1.se.conv_reduce.weight': np.float32(0.010736891), 'backbone.blocks.2.1.se.conv_reduce.bias': np.float32(0.44229928), 'backbone.blocks.2.1.se.conv_expand.weight': np.float32(0.0052833254), 'backbone.blocks.2.1.se.conv_expand.bias': np.float32(0.37494737), 'backbone.blocks.2.2.se.conv_reduce.weight': np.float32(0.01018576), 'backbone.blocks.2.2.se.conv_reduce.bias': np.float32(0.76548827), 'backbone.blocks.2.2.se.conv_expand.weight': np.float32(0.005852435), 'backbone.blocks.2.2.se.conv_expand.bias': np.float32(0.34305063), 'backbone.blocks.3.0.se.conv_reduce.weight': np.float32(0.005280867), 'backbone.blocks.3.0.se.conv_reduce.bias': np.float32(0.22363941), 'backbone.blocks.3.0.se.conv_expand.weight': np.float32(0.0055481154), 'backbone.blocks.3.0.se.conv_expand.bias': np.float32(0.35661826), 'backbone.blocks.3.1.se.conv_reduce.weight': np.float32(0.006267014), 'backbone.blocks.3.1.se.conv_reduce.bias': np.float32(0.16791363), 'backbone.blocks.3.1.se.conv_expand.weight': np.float32(0.004752126), 'backbone.blocks.3.1.se.conv_expand.bias': np.float32(0.4595384), 'backbone.blocks.3.2.se.conv_reduce.weight': np.float32(0.008043757), 'backbone.blocks.3.2.se.conv_reduce.bias': np.float32(0.7070179), 'backbone.blocks.3.2.se.conv_expand.weight': np.float32(0.0052997763), 'backbone.blocks.3.2.se.conv_expand.bias': np.float32(0.57014734), 'backbone.blocks.3.3.se.conv_reduce.weight': np.float32(0.0067717186), 'backbone.blocks.3.3.se.conv_reduce.bias': np.float32(0.8917818), 'backbone.blocks.3.3.se.conv_expand.weight': np.float32(0.004589464), 'backbone.blocks.3.3.se.conv_expand.bias': np.float32(0.5045715), 'backbone.blocks.4.0.se.conv_reduce.weight': np.float32(0.007108677), 'backbone.blocks.4.0.se.conv_reduce.bias': np.float32(0.2620472), 'backbone.blocks.4.0.se.conv_expand.weight': np.float32(0.005089943), 'backbone.blocks.4.0.se.conv_expand.bias': np.float32(0.4640806), 'backbone.blocks.4.1.se.conv_reduce.weight': np.float32(0.0069684894), 'backbone.blocks.4.1.se.conv_reduce.bias': np.float32(0.47875717), 'backbone.blocks.4.1.se.conv_expand.weight': np.float32(0.0042850357), 'backbone.blocks.4.1.se.conv_expand.bias': np.float32(0.26133552), 'backbone.blocks.4.2.se.conv_reduce.weight': np.float32(0.0073918095), 'backbone.blocks.4.2.se.conv_reduce.bias': np.float32(0.6097428), 'backbone.blocks.4.2.se.conv_expand.weight': np.float32(0.00478331), 'backbone.blocks.4.2.se.conv_expand.bias': np.float32(0.27567887), 'backbone.blocks.4.3.se.conv_reduce.weight': np.float32(0.00712973), 'backbone.blocks.4.3.se.conv_reduce.bias': np.float32(0.31541854), 'backbone.blocks.4.3.se.conv_expand.weight': np.float32(0.004829334), 'backbone.blocks.4.3.se.conv_expand.bias': np.float32(0.24276164), 'backbone.blocks.5.0.se.conv_reduce.weight': np.float32(0.0049907877), 'backbone.blocks.5.0.se.conv_reduce.bias': np.float32(0.12350295), 'backbone.blocks.5.0.se.conv_expand.weight': np.float32(0.004434649), 'backbone.blocks.5.0.se.conv_expand.bias': np.float32(0.3360125), 'backbone.blocks.5.1.se.conv_reduce.weight': np.float32(0.007018736), 'backbone.blocks.5.1.se.conv_reduce.bias': np.float32(0.32968292), 'backbone.blocks.5.1.se.conv_expand.weight': np.float32(0.004787285), 'backbone.blocks.5.1.se.conv_expand.bias': np.float32(0.17262083), 'backbone.blocks.5.2.se.conv_reduce.weight': np.float32(0.0067781573), 'backbone.blocks.5.2.se.conv_reduce.bias': np.float32(0.22329748), 'backbone.blocks.5.2.se.conv_expand.weight': np.float32(0.0050060963), 'backbone.blocks.5.2.se.conv_expand.bias': np.float32(0.16863412), 'backbone.blocks.5.3.se.conv_reduce.weight': np.float32(0.0064656613), 'backbone.blocks.5.3.se.conv_reduce.bias': np.float32(0.24503422), 'backbone.blocks.5.3.se.conv_expand.weight': np.float32(0.00516027), 'backbone.blocks.5.3.se.conv_expand.bias': np.float32(0.17906615), 'backbone.blocks.5.4.se.conv_reduce.weight': np.float32(0.005998645), 'backbone.blocks.5.4.se.conv_reduce.bias': np.float32(0.0844586), 'backbone.blocks.5.4.se.conv_expand.weight': np.float32(0.005243787), 'backbone.blocks.5.4.se.conv_expand.bias': np.float32(0.15984985), 'backbone.blocks.6.0.se.conv_reduce.weight': np.float32(0.0077620298), 'backbone.blocks.6.0.se.conv_reduce.bias': np.float32(0.06821143), 'backbone.blocks.6.0.se.conv_expand.weight': np.float32(0.0062629734), 'backbone.blocks.6.0.se.conv_expand.bias': np.float32(0.29103884), 'backbone.blocks.6.1.se.conv_reduce.weight': np.float32(0.006949202), 'backbone.blocks.6.1.se.conv_reduce.bias': np.float32(0.027424052), 'backbone.blocks.6.1.se.conv_expand.weight': np.float32(0.006183482), 'backbone.blocks.6.1.se.conv_expand.bias': np.float32(0.1473351), 'backbone.classifier.weight': np.float32(0.06470615), 'backbone.classifier.bias': np.float32(0.0), 'onnx::Conv_921': np.float32(0.15148622), 'onnx::Conv_922': np.float32(0.50986207), 'onnx::Conv_924': np.float32(4.7759175), 'onnx::Conv_925': np.float32(0.43270925), 'onnx::Conv_927': np.float32(0.062041357), 'onnx::Conv_928': np.float32(0.34950483), 'onnx::Conv_930': np.float32(0.38551536), 'onnx::Conv_931': np.float32(29.009254), 'onnx::Conv_933': np.float32(0.104856715), 'onnx::Conv_934': np.float32(0.5429738), 'onnx::Conv_936': np.float32(0.05832829), 'onnx::Conv_937': np.float32(1.0190755), 'onnx::Conv_939': np.float32(0.38107476), 'onnx::Conv_940': np.float32(0.3092984), 'onnx::Conv_942': np.float32(0.07388447), 'onnx::Conv_943': np.float32(0.3378015), 'onnx::Conv_945': np.float32(0.023448873), 'onnx::Conv_946': np.float32(0.56820667), 'onnx::Conv_948': np.float32(1.2538985), 'onnx::Conv_949': np.float32(0.599032), 'onnx::Conv_951': np.float32(0.028158017), 'onnx::Conv_952': np.float32(0.062150646), 'onnx::Conv_954': np.float32(0.01028979), 'onnx::Conv_955': np.float32(0.19251652), 'onnx::Conv_957': np.float32(0.6063421), 'onnx::Conv_958': np.float32(0.8575666), 'onnx::Conv_960': np.float32(0.04006331), 'onnx::Conv_961': np.float32(0.8243887), 'onnx::Conv_963': np.float32(0.009224776), 'onnx::Conv_964': np.float32(0.16485074), 'onnx::Conv_966': np.float32(0.15256289), 'onnx::Conv_967': np.float32(0.4627295), 'onnx::Conv_969': np.float32(0.120532006), 'onnx::Conv_970': np.float32(1.1225159), 'onnx::Conv_972': np.float32(0.008041562), 'onnx::Conv_973': np.float32(0.18765931), 'onnx::Conv_975': np.float32(0.42414623), 'onnx::Conv_976': np.float32(0.40028945), 'onnx::Conv_978': np.float32(0.023569696), 'onnx::Conv_979': np.float32(0.42046604), 'onnx::Conv_981': np.float32(0.00789499), 'onnx::Conv_982': np.float32(0.17707792), 'onnx::Conv_984': np.float32(0.30409363), 'onnx::Conv_985': np.float32(0.47685578), 'onnx::Conv_987': np.float32(0.046443395), 'onnx::Conv_988': np.float32(0.36639714), 'onnx::Conv_990': np.float32(0.0040740473), 'onnx::Conv_991': np.float32(0.24131082), 'onnx::Conv_993': np.float32(0.32484782), 'onnx::Conv_994': np.float32(0.5897301), 'onnx::Conv_996': np.float32(0.17842574), 'onnx::Conv_997': np.float32(1.7047879), 'onnx::Conv_999': np.float32(0.004858497), 'onnx::Conv_1000': np.float32(0.21873909), 'onnx::Conv_1002': np.float32(0.46897417), 'onnx::Conv_1003': np.float32(0.45151407), 'onnx::Conv_1005': np.float32(0.04507584), 'onnx::Conv_1006': np.float32(0.34606504), 'onnx::Conv_1008': np.float32(0.005153776), 'onnx::Conv_1009': np.float32(0.28836536), 'onnx::Conv_1011': np.float32(0.62673247), 'onnx::Conv_1012': np.float32(0.4859441), 'onnx::Conv_1014': np.float32(0.01527819), 'onnx::Conv_1015': np.float32(0.11452899), 'onnx::Conv_1017': np.float32(0.004929915), 'onnx::Conv_1018': np.float32(0.20815648), 'onnx::Conv_1020': np.float32(0.7160345), 'onnx::Conv_1021': np.float32(0.4576727), 'onnx::Conv_1023': np.float32(0.028785788), 'onnx::Conv_1024': np.float32(0.13067333), 'onnx::Conv_1026': np.float32(0.0039783567), 'onnx::Conv_1027': np.float32(0.13321547), 'onnx::Conv_1029': np.float32(0.3211377), 'onnx::Conv_1030': np.float32(0.5069599), 'onnx::Conv_1032': np.float32(0.04250753), 'onnx::Conv_1033': np.float32(0.61711967), 'onnx::Conv_1035': np.float32(0.0050845896), 'onnx::Conv_1036': np.float32(0.22420853), 'onnx::Conv_1038': np.float32(0.23493867), 'onnx::Conv_1039': np.float32(0.38840058), 'onnx::Conv_1041': np.float32(0.024242932), 'onnx::Conv_1042': np.float32(0.21693094), 'onnx::Conv_1044': np.float32(0.004610408), 'onnx::Conv_1045': np.float32(0.14695151), 'onnx::Conv_1047': np.float32(0.26091582), 'onnx::Conv_1048': np.float32(0.33047357), 'onnx::Conv_1050': np.float32(0.021223197), 'onnx::Conv_1051': np.float32(0.13547891), 'onnx::Conv_1053': np.float32(0.0045216926), 'onnx::Conv_1054': np.float32(0.17064108), 'onnx::Conv_1056': np.float32(0.24782512), 'onnx::Conv_1057': np.float32(0.36761308), 'onnx::Conv_1059': np.float32(0.027165458), 'onnx::Conv_1060': np.float32(0.14879373), 'onnx::Conv_1062': np.float32(0.0040934514), 'onnx::Conv_1063': np.float32(0.24798784), 'onnx::Conv_1065': np.float32(0.09911209), 'onnx::Conv_1066': np.float32(0.4681477), 'onnx::Conv_1068': np.float32(0.121244475), 'onnx::Conv_1069': np.float32(1.4648494), 'onnx::Conv_1071': np.float32(0.0029475635), 'onnx::Conv_1072': np.float32(0.19400777), 'onnx::Conv_1074': np.float32(0.2971234), 'onnx::Conv_1075': np.float32(0.45181212), 'onnx::Conv_1077': np.float32(0.01838679), 'onnx::Conv_1078': np.float32(0.08242127), 'onnx::Conv_1080': np.float32(0.0032598465), 'onnx::Conv_1081': np.float32(0.2097364), 'onnx::Conv_1083': np.float32(0.318609), 'onnx::Conv_1084': np.float32(0.3541227), 'onnx::Conv_1086': np.float32(0.016284617), 'onnx::Conv_1087': np.float32(0.074264266), 'onnx::Conv_1089': np.float32(0.0033583788), 'onnx::Conv_1090': np.float32(0.2498753), 'onnx::Conv_1092': np.float32(0.38081303), 'onnx::Conv_1093': np.float32(0.48028475), 'onnx::Conv_1095': np.float32(0.015479179), 'onnx::Conv_1096': np.float32(0.07732326), 'onnx::Conv_1098': np.float32(0.0034852242), 'onnx::Conv_1099': np.float32(0.2409522), 'onnx::Conv_1101': np.float32(0.41052207), 'onnx::Conv_1102': np.float32(0.54101753), 'onnx::Conv_1104': np.float32(0.013795876), 'onnx::Conv_1105': np.float32(0.084891796), 'onnx::Conv_1107': np.float32(0.0018651448), 'onnx::Conv_1108': np.float32(0.118858844), 'onnx::Conv_1110': np.float32(0.65086204), 'onnx::Conv_1111': np.float32(0.57505584), 'onnx::Conv_1113': np.float32(0.104650654), 'onnx::Conv_1114': np.float32(0.70088536), 'onnx::Conv_1116': np.float32(0.0023032627), 'onnx::Conv_1117': np.float32(0.20158407), 'onnx::Conv_1119': np.float32(1.326622), 'onnx::Conv_1120': np.float32(0.6218927), 'onnx::Conv_1122': np.float32(0.02642366), 'onnx::Conv_1123': np.float32(0.08153021), 'onnx::Conv_1125': np.float32(0.0068886178), 'onnx::Conv_1126': np.float32(0.7282017)}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compare_weights(model1_path, model2_path):\n",
    "    \"\"\"\n",
    "    Compare weights between two ONNX models.\n",
    "\n",
    "    Parameters:\n",
    "    - model1_path: Path to the first ONNX model (original).\n",
    "    - model2_path: Path to the second ONNX model (pruned).\n",
    "\n",
    "    Returns:\n",
    "    - weight_diff: A dictionary of weight differences.\n",
    "    \"\"\"\n",
    "    model1 = onnx.load(model1_path)\n",
    "    model2 = onnx.load(model2_path)\n",
    "\n",
    "    weight_diff = {}\n",
    "    for init1, init2 in zip(model1.graph.initializer, model2.graph.initializer):\n",
    "        name = init1.name\n",
    "        weights1 = np.frombuffer(init1.raw_data, dtype=np.float32)\n",
    "        weights2 = np.frombuffer(init2.raw_data, dtype=np.float32)\n",
    "\n",
    "        # Compute the difference\n",
    "        diff = np.abs(weights1 - weights2).mean()\n",
    "        weight_diff[name] = diff\n",
    "\n",
    "    return weight_diff\n",
    "\n",
    "\n",
    "# Compare weights\n",
    "weight_differences = compare_weights(\"../models/model.onnx\", \"pruned_model.onnx\")\n",
    "print(\"Weight Differences:\", weight_differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weight Name</th>\n",
       "      <th>Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>backbone.blocks.0.0.se.conv_reduce.weight</td>\n",
       "      <td>0.010197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>backbone.blocks.0.0.se.conv_reduce.bias</td>\n",
       "      <td>0.279723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>backbone.blocks.0.0.se.conv_expand.weight</td>\n",
       "      <td>0.009743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>backbone.blocks.0.0.se.conv_expand.bias</td>\n",
       "      <td>0.312522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>backbone.blocks.0.1.se.conv_reduce.weight</td>\n",
       "      <td>0.009796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>onnx::Conv_1120</td>\n",
       "      <td>0.621893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>onnx::Conv_1122</td>\n",
       "      <td>0.026424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>onnx::Conv_1123</td>\n",
       "      <td>0.081530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>onnx::Conv_1125</td>\n",
       "      <td>0.006889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>onnx::Conv_1126</td>\n",
       "      <td>0.728202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>232 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Weight Name  Difference\n",
       "0    backbone.blocks.0.0.se.conv_reduce.weight    0.010197\n",
       "1      backbone.blocks.0.0.se.conv_reduce.bias    0.279723\n",
       "2    backbone.blocks.0.0.se.conv_expand.weight    0.009743\n",
       "3      backbone.blocks.0.0.se.conv_expand.bias    0.312522\n",
       "4    backbone.blocks.0.1.se.conv_reduce.weight    0.009796\n",
       "..                                         ...         ...\n",
       "227                            onnx::Conv_1120    0.621893\n",
       "228                            onnx::Conv_1122    0.026424\n",
       "229                            onnx::Conv_1123    0.081530\n",
       "230                            onnx::Conv_1125    0.006889\n",
       "231                            onnx::Conv_1126    0.728202\n",
       "\n",
       "[232 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "diff_df = pd.DataFrame(weight_differences.items(), columns=[\"Weight Name\", \"Difference\"])\n",
    "diff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model Nodes: 342\n",
      "Pruned Model Nodes: 342\n",
      "Removed Nodes: set()\n",
      "Added Nodes: set()\n"
     ]
    }
   ],
   "source": [
    "def compare_model_structure(model1_path, model2_path):\n",
    "    \"\"\"\n",
    "    Compare the structure of two ONNX models.\n",
    "\n",
    "    Parameters:\n",
    "    - model1_path: Path to the first ONNX model (original).\n",
    "    - model2_path: Path to the second ONNX model (pruned).\n",
    "    \"\"\"\n",
    "    model1 = onnx.load(model1_path)\n",
    "    model2 = onnx.load(model2_path)\n",
    "\n",
    "    # Count nodes and layers\n",
    "    nodes1 = len(model1.graph.node)\n",
    "    nodes2 = len(model2.graph.node)\n",
    "    print(f\"Original Model Nodes: {nodes1}\")\n",
    "    print(f\"Pruned Model Nodes: {nodes2}\")\n",
    "\n",
    "    # Check node differences\n",
    "    original_nodes = {node.name for node in model1.graph.node}\n",
    "    pruned_nodes = {node.name for node in model2.graph.node}\n",
    "\n",
    "    removed_nodes = original_nodes - pruned_nodes\n",
    "    added_nodes = pruned_nodes - original_nodes\n",
    "\n",
    "    print(\"Removed Nodes:\", removed_nodes)\n",
    "    print(\"Added Nodes:\", added_nodes)\n",
    "\n",
    "\n",
    "# Compare structure\n",
    "compare_model_structure(\"../models/model.onnx\", \"pruned_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Difference: 0.36133397\n",
      "Output 1: [[0.6386661 0.361334 ]]\n",
      "Output 2: [[1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def compare_model_outputs(model1_path, model2_path, input_data):\n",
    "    \"\"\"\n",
    "    Compare the outputs of two ONNX models for the same input.\n",
    "\n",
    "    Parameters:\n",
    "    - model1_path: Path to the first ONNX model (original).\n",
    "    - model2_path: Path to the second ONNX model (pruned).\n",
    "    - input_data: Input data for inference.\n",
    "\n",
    "    Returns:\n",
    "    - output_diff: Mean absolute difference between the outputs.\n",
    "    \"\"\"\n",
    "    session1 = ort.InferenceSession(model1_path)\n",
    "    session2 = ort.InferenceSession(model2_path)\n",
    "\n",
    "    input_name1 = session1.get_inputs()[0].name\n",
    "    input_name2 = session2.get_inputs()[0].name\n",
    "\n",
    "    output1 = session1.run(None, {input_name1: input_data})[0]\n",
    "    output2 = session2.run(None, {input_name2: input_data})[0]\n",
    "\n",
    "    # Compute difference\n",
    "    diff = np.abs(output1 - output2).mean()\n",
    "    return diff, output1, output2\n",
    "\n",
    "\n",
    "# Prepare input data\n",
    "input_shape = ort.InferenceSession(\"../models/model.onnx\").get_inputs()[0].shape\n",
    "input_data = np.random.rand(*input_shape).astype(np.float32)\n",
    "\n",
    "# Compare outputs\n",
    "output_difference, out1, out2 = compare_model_outputs(\"../models/model.onnx\", \"optimized_model.onnx\", input_data)\n",
    "print(\"Output Difference:\", output_difference)\n",
    "print(\"Output 1:\", out1)\n",
    "print(\"Output 2:\", out2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plants",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
