{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#documentation","title":"Documentation","text":"<p>Documentation for plant_leaves</p>"},{"location":"#plant_leaves.data","title":"plant_leaves.data","text":""},{"location":"#plant_leaves.data.load_processed_data","title":"load_processed_data","text":"<pre><code>load_processed_data(processed_data_path: Path) -&gt; Tuple[torch.utils.data.TensorDataset, torch.utils.data.TensorDataset, torch.utils.data.TensorDataset]\n</code></pre> <p>Load the processed datasets and targets.</p> <p>Parameters: - processed_data_path: Path to the folder containing processed data.</p> <p>Returns: - Tuple of three torch.utils.data.TensorDataset objects: train, test, validation</p> Source code in <code>plant_leaves/data.py</code> <pre><code>@logger.catch()\ndef load_processed_data(\n    processed_data_path: Path,\n) -&gt; Tuple[torch.utils.data.TensorDataset, torch.utils.data.TensorDataset, torch.utils.data.TensorDataset]:\n    \"\"\"\n    Load the processed datasets and targets.\n\n    Parameters:\n    - processed_data_path: Path to the folder containing processed data.\n\n    Returns:\n    - Tuple of three torch.utils.data.TensorDataset objects: train, test, validation\n    \"\"\"\n    logger.configure(extra={\"prefix\": LOG_PREFIX})\n\n    logger.info(f\"Loading processed data...\")\n    logger.info(f\"Searching for data at: {processed_data_path}\")\n    # Load the processed datasets and targets\n    train_images = torch.load(processed_data_path / \"train\" / \"datasets.pt\", weights_only=True)\n    train_target = torch.load(processed_data_path / \"train\" / \"targets.pt\", weights_only=True)\n    test_images = torch.load(processed_data_path / \"test\" / \"datasets.pt\", weights_only=True)\n    test_target = torch.load(processed_data_path / \"test\" / \"targets.pt\", weights_only=True)\n    validation_images = torch.load(processed_data_path / \"valid\" / \"datasets.pt\", weights_only=True)\n    validation_target = torch.load(processed_data_path / \"valid\" / \"targets.pt\", weights_only=True)\n\n    # Create the joint datasets with targets\n    train = torch.utils.data.TensorDataset(train_images, train_target)\n    test = torch.utils.data.TensorDataset(test_images, test_target)\n    validation = torch.utils.data.TensorDataset(validation_images, validation_target)\n\n    # Create the joint datasets with targets\n    train = torch.utils.data.TensorDataset(train_images, train_target)\n    test = torch.utils.data.TensorDataset(test_images, test_target)\n    validation = torch.utils.data.TensorDataset(validation_images, validation_target)\n\n    # # Set a random seed for reproducibility\n    # np.random.seed(42)\n\n    # # TODO: Remove this function after checking the code\n    # # Function to create a 5% subset\n    # def create_subset(dataset, fraction=0.05):\n    #     subset_size = int(fraction * len(dataset))  # Calculate 5% of the dataset size\n    #     indices = np.random.choice(len(dataset), subset_size, replace=False)  # Randomly select indices\n    #     return Subset(dataset, indices)\n\n    # # Create 5% subsets\n    # train_subset = create_subset(train)\n    # test_subset = create_subset(test)\n    # validation_subset = create_subset(validation)\n\n    # return train_subset, test_subset, validation_subset #TODO: Replace with train, test, validation after checking the code\n    return train, test, validation\n</code></pre>"},{"location":"#plant_leaves.data.main_preprocessing","title":"main_preprocessing","text":"<pre><code>main_preprocessing(data_path: Path, output_path: Path, dimensions: Tuple[int, int] = (240, 240)) -&gt; None\n</code></pre> <p>Output two folders for each category in the dataset respectively.</p> <p>Parameters: - data_path: Path to the folder containing raw data. - output_path: Path to the folder where processed data will be stored. - dimensions: Target dimensions for image resizing (width, height).</p> <p>Returns: - None</p> Source code in <code>plant_leaves/data.py</code> <pre><code>def main_preprocessing(data_path: Path, output_path: Path, dimensions: Tuple[int, int] = (240, 240)) -&gt; None:\n    \"\"\"\n    Output two folders for each category in the dataset respectively.\n\n    Parameters:\n    - data_path: Path to the folder containing raw data.\n    - output_path: Path to the folder where processed data will be stored.\n    - dimensions: Target dimensions for image resizing (width, height).\n\n    Returns:\n    - None\n    \"\"\"\n    logger.configure(extra={\"prefix\": LOG_PREFIX})\n\n    transform = transforms.Compose(\n        [\n            transforms.Resize(dimensions),\n            transforms.ToTensor(),\n        ]\n    )\n    datasets_pt_l, targets_pt_l = [], []\n\n    # Extract images to output folders\n    for folder_path in data_path.iterdir():\n        if not folder_path.is_dir() or folder_path.name.startswith(\".\"):\n            continue  # Skip files and hidden folders\n\n        for img_path in folder_path.iterdir():\n            if not img_path.is_file() or img_path.name.startswith(\".\"):\n                continue  # Skip hidden files and non-image files\n\n            if img_path.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]:\n                category = \"healthy\" if \"healthy\" in folder_path.name else \"diseased\"\n\n                try:\n                    img = Image.open(img_path)\n\n                    # Apply transformations\n                    tensor = transform(img)\n\n                    if category == \"healthy\":\n                        datasets_pt_l.append(tensor)\n                        targets_pt_l.append(0)\n                    else:\n                        datasets_pt_l.append(tensor)\n                        targets_pt_l.append(1)\n\n                except Exception as e:\n                    logger.error(f\"Error processing image {img_path}: {e}\")\n                    continue\n\n    # Transform lists to tensors\n    datasets_pt = torch.stack(datasets_pt_l)\n    targets_pt = torch.tensor(targets_pt_l)\n\n    # Normalize the datasets\n    datasets_pt = normalize(datasets_pt)\n\n    # Ensure the output directory exists\n    output_path.mkdir(parents=True, exist_ok=True)\n\n    # Save the datasets and targets as .pt files\n    torch.save(datasets_pt, output_path / \"datasets.pt\")\n    torch.save(targets_pt, output_path / \"targets.pt\")\n\n    logger.info(f\"Datasets saved in {output_path}\")\n</code></pre>"},{"location":"#plant_leaves.data.normalize","title":"normalize","text":"<pre><code>normalize(images: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Normalize images as (X - mean(X)) / std(X).</p> <p>Parameters: - images: Tensor of shape (N, 3, 240, 240)</p> <p>Returns: - Normalized images</p> Source code in <code>plant_leaves/data.py</code> <pre><code>def normalize(images: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Normalize images as (X - mean(X)) / std(X).\n\n    Parameters:\n    - images: Tensor of shape (N, 3, 240, 240)\n\n    Returns:\n    - Normalized images\n    \"\"\"\n    return (images - images.mean()) / images.std()\n</code></pre>"},{"location":"#plant_leaves.data.preprocess","title":"preprocess","text":"<pre><code>preprocess(raw_data_path: Path = typer.Argument(default=Path('data/raw/plant-leaves-for-image-classification/Plants_2'), help='Path to the folder containing raw data.'), output_folder: Path = typer.Argument(default=Path('data/processed'), help='Path to the folder where processed data will be stored.'), dimensions: Tuple[int, int] = typer.Option((240, 240), help='Target dimensions for image resizing (width, height).')) -&gt; None\n</code></pre> <p>Preprocess the raw data and save the processed data in the output folder.</p> <pre><code>Parameters:\n    raw_data_path: Path to the folder containing raw data.\n    output_folder: Path to the folder where processed data will be stored.\n    dimensions: Target dimensions for image resizing (width, height).\n\nReturns:\n    None\n</code></pre> Source code in <code>plant_leaves/data.py</code> <pre><code>@data_typer.command()\n@logger.catch()\ndef preprocess(\n    raw_data_path: Path = typer.Argument(\n        default=Path(\"data/raw/plant-leaves-for-image-classification/Plants_2\"),\n        help=\"Path to the folder containing raw data.\",\n    ),\n    output_folder: Path = typer.Argument(\n        default=Path(\"data/processed\"), help=\"Path to the folder where processed data will be stored.\"\n    ),\n    dimensions: Tuple[int, int] = typer.Option(\n        (240, 240), help=\"Target dimensions for image resizing (width, height).\"\n    ),\n) -&gt; None:\n    \"\"\"\n    Preprocess the raw data and save the processed data in the output folder.\n\n        Parameters:\n            raw_data_path: Path to the folder containing raw data.\n            output_folder: Path to the folder where processed data will be stored.\n            dimensions: Target dimensions for image resizing (width, height).\n\n        Returns:\n            None\n    \"\"\"\n    logger.configure(extra={\"prefix\": LOG_PREFIX})\n\n    logger.info(f\"Checking if raw data folder exists...\")\n    try:\n        if not raw_data_path.exists():\n            logger.info(f\"The raw data folder does not exist. Downloading the dataset...\")\n            download_dataset()  # TODO: Add arguments properly. Issue: raw_data_path links to data folder and not to cookie cutter raw data folder.\n    except Exception as e:  # If the download failed, exit.\n        logger.error(\"Download failed. Exiting...\")\n        exit()\n\n    logger.info(f\"Preprocessing data...\")\n    for dataset_path in raw_data_path.iterdir():\n        # Skip hidden folders and the \"images to predict\" folder\n        if dataset_path.name.startswith(\".\") or dataset_path.name == \"images to predict\":\n            continue\n        # Ensure it is a directory\n        if not dataset_path.is_dir():\n            continue\n        # Check if the folder exists and if not create it\n        output_subfolder = output_folder / dataset_path.name\n        if not output_subfolder.exists():\n            output_subfolder.mkdir(parents=True)\n\n        logger.info(f\"Dataset path : {dataset_path} \\n Output path : {output_subfolder}\")\n        # Call the preprocessing function\n        main_preprocessing(dataset_path, output_subfolder)\n</code></pre>"},{"location":"#plant_leaves.model","title":"plant_leaves.model","text":"<p>Module defining a model predicting the healthiness of plant leaves.</p>"},{"location":"#plant_leaves.model.PlantClassifier","title":"PlantClassifier","text":"<p>               Bases: <code>Module</code></p> <p>Neural network using a pre-trained backbone with a custom head for predicting 2 classes of plant leaves</p> Source code in <code>plant_leaves/model.py</code> <pre><code>class PlantClassifier(torch.nn.Module):\n    \"\"\"Neural network using a pre-trained backbone\n    with a custom head for predicting 2 classes\n    of plant leaves\n\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.backbone = timm.create_model(\"efficientnet_b1.ra4_e3600_r240_in1k\", pretrained=True, num_classes=2)\n\n    def forward(self, x: torch.Tensor) -&gt; Any:\n        \"\"\"Executes a forward pass on input data\n\n        Args:\n            x (torch.Tensor): Input with shape [batch_size n_channels width height],\n                where n_channels == 3\n\n        Returns:\n            Any: predictions shape [batch_size n_classes]\n        \"\"\"\n        hidden = self.backbone(x)\n        predictions = F.softmax(hidden, dim=1)\n        return predictions\n</code></pre>"},{"location":"#plant_leaves.model.PlantClassifier.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Any\n</code></pre> <p>Executes a forward pass on input data</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input with shape [batch_size n_channels width height], where n_channels == 3</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>predictions shape [batch_size n_classes]</p> Source code in <code>plant_leaves/model.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; Any:\n    \"\"\"Executes a forward pass on input data\n\n    Args:\n        x (torch.Tensor): Input with shape [batch_size n_channels width height],\n            where n_channels == 3\n\n    Returns:\n        Any: predictions shape [batch_size n_classes]\n    \"\"\"\n    hidden = self.backbone(x)\n    predictions = F.softmax(hidden, dim=1)\n    return predictions\n</code></pre>"},{"location":"#plant_leaves.evaluate","title":"plant_leaves.evaluate","text":""},{"location":"#plant_leaves.evaluate.evaluate","title":"evaluate","text":"<pre><code>evaluate(model_checkpoint: str) -&gt; None\n</code></pre> <p>Takes a pretrained CNN model and evaluates it on test data</p> <pre><code>    Parameters:\n                model_checkpoint (str): a PathLike object containing a file name\n</code></pre> Source code in <code>plant_leaves/evaluate.py</code> <pre><code>def evaluate(model_checkpoint: str) -&gt; None:\n    \"\"\"\n    Takes a pretrained CNN model and evaluates it on test data\n\n            Parameters:\n                        model_checkpoint (str): a PathLike object containing a file name\n\n    \"\"\"\n    logger.configure(extra={\"prefix\": LOG_PREFIX})\n\n    print(model_checkpoint)\n    model = PlantClassifier().to(DEVICE)\n    model.load_state_dict(torch.load(model_checkpoint))\n\n    _, test_set, _ = load_processed_data()\n\n    logger.info(f\"Train set size: {len(test_set)}\")\n\n    test_dataloader = DataLoader(test_set, batch_size=64)\n\n    model.eval()\n    correct, total = 0, 0\n\n    for img, target in test_dataloader:\n        logger.info(\"Initiate model evaluation on test data...\")\n        img, target = img.to(DEVICE), target.to(DEVICE)\n        y_pred = model(img)\n        correct += (y_pred.argmax(dim=1) == target).float().sum().item()\n        total += target.size(0)\n    logger.info(f\"Test accuracy: {correct / total}\")\n</code></pre>"},{"location":"#plant_leaves.data_stats","title":"plant_leaves.data_stats","text":""},{"location":"#plant_leaves.data_stats.dataset_statistics","title":"dataset_statistics","text":"<pre><code>dataset_statistics(datadir: str = 'data/processed/') -&gt; None\n</code></pre> <p>Compute dataset statistics.</p> Source code in <code>plant_leaves/data_stats.py</code> <pre><code>def dataset_statistics(datadir: str = \"data/processed/\") -&gt; None:\n    \"\"\"Compute dataset statistics.\"\"\"\n    root_dir = Path(get_project_root())\n    logger.configure(extra={\"prefix\": LOG_PREFIX})\n    try:\n        train_dataset, test_dataset, _ = load_processed_data(Path(f\"{root_dir}/{datadir}\"))\n    except FileNotFoundError:\n        logger.error(f\"No processed data found at {datadir}\")\n        exit(1)\n\n    print(f\"------- Train dataset -------\")\n    print(f\"Number of images: {len(train_dataset)}\")\n    print(f\"Image shape: {train_dataset[0][0].shape}\")\n    print(\"\\n\")\n    print(f\"------- Test dataset -------\")\n    print(f\"Number of images: {len(test_dataset)}\")\n    print(f\"Image shape: {test_dataset[0][0].shape}\")\n\n    # train_dataloader = DataLoader(dataset=train_dataset, batch_size=32)\n    train_images = [train_dataset[i][0] for i in range(len(train_dataset))]\n    train_labels = [train_dataset[i][1] for i in range(len(train_dataset))]\n    test_images = [test_dataset[i][0] for i in range(len(test_dataset))]\n    test_labels = [test_dataset[i][1] for i in range(len(test_dataset))]\n\n    show_image_and_target([image.permute(1, 2, 0) for image in train_images[:32]], train_labels[:32], show=False)\n    plt.savefig(f\"{root_dir}/outputs/images/plant_leaves.png\")\n    plt.close()\n\n    train_label_distribution = torch.bincount(torch.stack(train_labels))\n    test_label_distribution = torch.bincount(torch.stack(test_labels))\n\n    plt.bar(torch.arange(10), train_label_distribution)\n    plt.title(\"Train label distribution\")\n    plt.xlabel(\"Label\")\n    plt.ylabel(\"Count\")\n    plt.savefig(\"train_label_distribution.png\")\n    plt.close()\n\n    plt.bar(torch.arange(10), test_label_distribution)\n    plt.title(\"Test label distribution\")\n    plt.xlabel(\"Label\")\n    plt.ylabel(\"Count\")\n    plt.savefig(\"test_label_distribution.png\")\n    plt.close()\n</code></pre>"},{"location":"#plant_leaves.api","title":"plant_leaves.api","text":""},{"location":"#plant_leaves.api.lifespan","title":"lifespan  <code>async</code>","text":"<pre><code>lifespan(app: FastAPI) -&gt; AsyncGenerator[None, None]\n</code></pre> <p>Load and clean up the model on startup and shutdown.</p> <p>Parameters: - app: FastAPI application instance.</p> <p>Yields: - None: Context manager for the lifespan of the application.</p> Source code in <code>plant_leaves/api.py</code> <pre><code>@asynccontextmanager\nasync def lifespan(app: FastAPI) -&gt; AsyncGenerator[None, None]:\n    \"\"\"\n    Load and clean up the model on startup and shutdown.\n\n    Parameters:\n    - app: FastAPI application instance.\n\n    Yields:\n    - None: Context manager for the lifespan of the application.\n    \"\"\"\n    global model\n    model_path = os.getenv(\"MODEL_PATH\", \"models/model.onnx\")\n    model = onnx.load(model_path)\n    onnx.checker.check_model(model)\n    yield\n\n    print(\"Cleaning up\")\n    del model\n</code></pre>"},{"location":"#plant_leaves.api.normalize","title":"normalize","text":"<pre><code>normalize(images: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Normalize images as (X - mean(X)) / std(X).</p> <p>Parameters: - images: Tensor of shape (N, 3, 240, 240).</p> <p>Returns: - torch.Tensor: Normalized tensor of images.</p> Source code in <code>plant_leaves/api.py</code> <pre><code>def normalize(images: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Normalize images as (X - mean(X)) / std(X).\n\n    Parameters:\n    - images: Tensor of shape (N, 3, 240, 240).\n\n    Returns:\n    - torch.Tensor: Normalized tensor of images.\n    \"\"\"\n    return (images - images.mean()) / images.std()\n</code></pre>"},{"location":"#plant_leaves.api.predict","title":"predict  <code>async</code>","text":"<pre><code>predict(data: UploadFile = File(...)) -&gt; PredictionResponse\n</code></pre> <p>Predict whether the image is healthy or diseased.</p> Source code in <code>plant_leaves/api.py</code> <pre><code>@app.post(\"/predict/\", response_model=PredictionResponse)\nasync def predict(data: UploadFile = File(...)) -&gt; PredictionResponse:\n    \"\"\"\n    Predict whether the image is healthy or diseased.\n    \"\"\"\n    request_counter.inc()\n    with request_latency.time():\n        try:\n            image_array = await preprocess_image(data)\n            review_summary.observe(image_array.size)\n            model_path = os.getenv(\"MODEL_PATH\", \"models/model.onnx\")\n            ort_sess = ort.InferenceSession(model_path)\n            y_pred = ort_sess.run(None, {\"input\": image_array})\n            prediction_idx = y_pred[0][0].argmax(0)\n            label = \"healthy\" if prediction_idx.item() == 0 else \"diseased\"\n            confidence = y_pred[0][0][prediction_idx].item()\n            print(y_pred, prediction_idx, label, confidence)\n            return PredictionResponse(image_label=label, confidence=confidence, status_code=200)\n\n        except Exception as e:\n            error_counter.inc()\n            print(f\"Error: {e}\")\n            return PredictionResponse(image_label=\"error\", confidence=0.0, status_code=500)\n</code></pre>"},{"location":"#plant_leaves.api.preprocess_image","title":"preprocess_image  <code>async</code>","text":"<pre><code>preprocess_image(data: UploadFile) -&gt; np.ndarray\n</code></pre> <p>Transforms an image file into a normalized tensor.</p> <p>Parameters: - data: Uploaded image file.</p> <p>Returns: - torch.Tensor: Normalized tensor representation of the image.</p> Source code in <code>plant_leaves/api.py</code> <pre><code>async def preprocess_image(data: UploadFile) -&gt; np.ndarray:\n    \"\"\"\n    Transforms an image file into a normalized tensor.\n\n    Parameters:\n    - data: Uploaded image file.\n\n    Returns:\n    - torch.Tensor: Normalized tensor representation of the image.\n    \"\"\"\n\n    contents = await data.read()\n    img = Image.open(io.BytesIO(contents))\n\n    # Use io.BytesIO for in-memory binary streams\n    transform = transforms.Compose(\n        [\n            transforms.Resize((240, 240)),  # TODO: Resize to 240x240 after re-training the model\n            transforms.ToTensor(),  # Convert to tensor\n        ]\n    )\n    tensor = transform(img)\n    norm_tensor = normalize(tensor)\n    norm_tensor = torch.unsqueeze(norm_tensor, 0)\n\n    return norm_tensor.numpy()\n</code></pre>"},{"location":"#plant_leaves.api.root","title":"root","text":"<pre><code>root() -&gt; Dict[str, str | HTTPStatus]\n</code></pre> <p>Health check endpoint.</p> <p>Returns: - response: Dictionary containing a message and status code.</p> Source code in <code>plant_leaves/api.py</code> <pre><code>@app.get(\"/\")\ndef root() -&gt; Dict[str, str | HTTPStatus]:\n    \"\"\"\n    Health check endpoint.\n\n    Returns:\n    - response: Dictionary containing a message and status code.\n    \"\"\"\n    response = {\n        \"message\": HTTPStatus.OK.phrase,\n        \"status-code\": HTTPStatus.OK,\n    }\n    return response\n</code></pre>"},{"location":"#plant_leaves.train_wandb","title":"plant_leaves.train_wandb","text":""},{"location":"#plant_leaves.train_wandb.train","title":"train","text":"<pre><code>train(lr: float = 0.001, batch_size: int = 32, epochs: int = 5, num_workers: int = 1) -&gt; None\n</code></pre> <p>Takes the CNN model and performs the training process</p> <pre><code>    Parameters:\n                batch_size (int): size of training batches\n                epochs (int): number of training runs\n                lr (float): learning rate of optimizer\n</code></pre> Source code in <code>plant_leaves/train_wandb.py</code> <pre><code>def train(lr: float = 0.001, batch_size: int = 32, epochs: int = 5, num_workers: int = 1) -&gt; None:\n    \"\"\"\n    Takes the CNN model and performs the training process\n\n            Parameters:\n                        batch_size (int): size of training batches\n                        epochs (int): number of training runs\n                        lr (float): learning rate of optimizer\n\n    \"\"\"\n    # Load WandB environment variables\n    wandb_api_key = os.getenv(\"WANDB_API_KEY\")\n    wandb_project = os.getenv(\"WANDB_PROJECT\")\n    wandb_entity = os.getenv(\"WANDB_ENTITY\")\n\n    if not all([wandb_api_key, wandb_project, wandb_entity]):\n        logger.info(\n            \"Please set WANDB_API_KEY, WANDB_PROJECT, and WANDB_ENTITY in the environment variables\"\n        )  # logged as DEBUG\n        mode = \"disabled\"\n    else:\n        logger.info(f\"Logging in with api key to project {wandb_project} for entity {wandb_entity}\")  # logged as DEBUG\n        wandb.login(key=wandb_api_key)\n        mode = \"online\"\n\n    current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    run_name = f\"train_{current_time}_lr_{lr}_bs_{batch_size}_epochs_{epochs}\"\n    run = wandb.init(\n        project=wandb_project,  # Group all experiments for this project\n        entity=wandb_entity,  # Specify the team or user account\n        job_type=\"train\",  # Specify the type of job\n        name=run_name,\n        config={\"lr\": lr, \"batch_size\": batch_size, \"epochs\": epochs},\n        mode=mode,\n    )\n\n    model = PlantClassifier().to(DEVICE)\n    train_set, _, validation_set = load_processed_data(DATA_PATH)\n\n    logger.configure(extra={\"prefix\": LOG_PREFIX})\n    logger.info(f\"Train set size: {len(train_set)}\")\n    logger.info(f\"Validation set size: {len(validation_set)}\")\n\n    train_dataloader = DataLoader(dataset=train_set, batch_size=batch_size, num_workers=num_workers)\n    val_dataloader = DataLoader(dataset=validation_set, batch_size=batch_size, num_workers=num_workers)\n\n    loss_fn = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    statistics: Dict[str, list[float]] = {\"train_loss\": [], \"train_accuracy\": [], \"val_loss\": [], \"val_accuracy\": []}\n\n    for epoch in range(epochs):\n        model.train()\n        epoch_loss, epoch_accuracy = 0.0, 0.0\n        for i, (img, target) in enumerate(train_dataloader):\n            img, target = img.to(DEVICE), target.to(DEVICE)\n            optimizer.zero_grad()\n            y_pred = model(img)\n            loss = loss_fn(y_pred, target)\n            loss.backward()\n            optimizer.step()\n            statistics[\"train_loss\"].append(loss.item())\n            epoch_loss += loss.item()\n            accuracy = (y_pred.argmax(dim=1) == target).float().mean().item()\n            epoch_accuracy += accuracy\n            statistics[\"train_accuracy\"].append(accuracy)\n\n            wandb.log({\"train_loss\": loss.item(), \"train_accuracy\": accuracy})\n            if i % 100 == 0:\n                logger.info(f\"Epoch {epoch}, iter {i}, loss: {loss.item()}\")\n\n        epoch_loss = epoch_loss / len(train_dataloader)\n        epoch_accuracy = epoch_accuracy / len(train_set)\n\n        logger.info(f\"Epoch {epoch}, loss: {epoch_loss}, accuracy: {epoch_accuracy}\")\n        wandb.log({\"epoch_train_loss\": epoch_loss, \"epoch_train_accuracy\": epoch_accuracy})\n\n        # Validation loop\n        model.eval()\n        val_loss, val_correct = 0.0, 0\n\n        with torch.no_grad():\n            for img, target in val_dataloader:\n                img, target = img.to(DEVICE), target.to(DEVICE)\n                y_pred = model(img)\n                loss = loss_fn(y_pred, target)\n                val_loss += loss.item()\n                val_correct += (y_pred.argmax(dim=1) == target).float().sum().item()\n\n        val_loss /= len(val_dataloader)\n        val_accuracy = val_correct / len(validation_set)\n        statistics[\"val_loss\"].append(val_loss)\n        statistics[\"val_accuracy\"].append(val_accuracy)\n\n        wandb.log({\"val_loss\": val_loss, \"val_accuracy\": val_accuracy})\n        logger.info(f\"Validation loss: {val_loss}, accuracy: {val_accuracy}\")\n\n    logger.info(\"Training complete\")\n    model_path = \"models/model.pth\"\n    torch.save(model.state_dict(), model_path)\n    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n    axs[0].plot(statistics[\"train_loss\"])\n    axs[0].set_title(\"Train loss\")\n    axs[1].plot(statistics[\"train_accuracy\"])\n    axs[1].set_title(\"Train accuracy\")\n    fig_path = \"reports/figures/training_statistics.png\"\n    fig.savefig(fig_path)\n    run.log({\"training_statistics_via_matplotlib\": wandb.Image(fig_path)})\n\n    # Log model to wandb\n    artifact = wandb.Artifact(\n        name=\"leaf_classifier\",\n        type=\"model\",\n        description=\"A model trained to classify healthy and diseased plant leaves\",\n        metadata={\"accuracy\": statistics[\"train_accuracy\"][-1], \"loss\": statistics[\"train_loss\"][-1]},\n    )\n    artifact.add_file(model_path)\n    run.log_artifact(artifact)\n\n    run.finish()\n</code></pre>"},{"location":"#plant_leaves.train","title":"plant_leaves.train","text":""},{"location":"#plant_leaves.train.train","title":"train","text":"<pre><code>train(cfg: DictConfig) -&gt; None\n</code></pre> <p>Takes the CNN model and performs the training process</p> <pre><code>    Parameters:\n                cfg: Configuration object fetched from hydra\n</code></pre> Source code in <code>plant_leaves/train.py</code> <pre><code>@hydra.main(\n    config_path=os.path.join(PROJECT_ROOT, \"configs\"),\n    config_name=\"default_config.yaml\",\n    version_base=None,\n)\ndef train(cfg: DictConfig) -&gt; None:\n    \"\"\"\n    Takes the CNN model and performs the training process\n\n            Parameters:\n                        cfg: Configuration object fetched from hydra\n\n    \"\"\"\n    params = cfg.experiment\n\n    # Load WandB environment variables\n    wandb_api_key = os.getenv(\"WANDB_API_KEY\")\n    wandb_project = os.getenv(\"WANDB_PROJECT\")\n    wandb_entity = os.getenv(\"WANDB_ENTITY\")\n\n    if not all([wandb_api_key, wandb_project, wandb_entity]):\n        logger.info(\n            \"Please set WANDB_API_KEY, WANDB_PROJECT, and WANDB_ENTITY in the environment variables\"\n        )  # logged as DEBUG\n        mode = \"disabled\"\n    else:\n        logger.info(f\"Logging in with api key to project {wandb_project} for entity {wandb_entity}\")  # logged as DEBUG\n        wandb.login(key=wandb_api_key)\n        mode = \"online\"\n\n    current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    run_name = f\"train_{current_time}_lr_{params.lr}_bs_{params.batch_size}_epochs_{params.epochs}\"\n    run = wandb.init(\n        project=wandb_project,  # Group all experiments for this project\n        entity=wandb_entity,  # Specify the team or user account\n        job_type=\"train\",  # Specify the type of job\n        name=run_name,\n        config={\"lr\": params.lr, \"batch_size\": params.batch_size, \"epochs\": params.epochs},\n        mode=mode,\n    )\n\n    model = PlantClassifier().to(DEVICE)\n    train_set, _, validation_set = load_processed_data(DATA_PATH)\n\n    logger.configure(extra={\"prefix\": LOG_PREFIX})\n    logger.info(f\"Train set size: {len(train_set)}\")\n    logger.info(f\"Validation set size: {len(validation_set)}\")\n\n    # train_dataloader = DataLoader(dataset=train_set, batch_size=params.batch_size)\n    train_dataloader = DataLoader(dataset=train_set, batch_size=params.batch_size, num_workers=params.num_workers)\n    val_dataloader = DataLoader(dataset=validation_set, batch_size=params.batch_size, num_workers=params.num_workers)\n\n    loss_fn = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=params.lr)\n\n    statistics: Dict[str, list[float]] = {\"train_loss\": [], \"train_accuracy\": [], \"val_loss\": [], \"val_accuracy\": []}\n\n    for epoch in range(params.epochs):\n        model.train()\n        epoch_loss, epoch_accuracy = 0.0, 0.0\n        for i, (img, target) in enumerate(train_dataloader):\n            img, target = img.to(DEVICE), target.to(DEVICE)\n            optimizer.zero_grad()\n            y_pred = model(img)\n            loss = loss_fn(y_pred, target)\n            loss.backward()\n            optimizer.step()\n            statistics[\"train_loss\"].append(loss.item())\n            epoch_loss += loss.item()\n            accuracy = (y_pred.argmax(dim=1) == target).float().mean().item()\n            epoch_accuracy += accuracy\n            statistics[\"train_accuracy\"].append(accuracy)\n\n            wandb.log({\"train_loss\": loss.item(), \"train_accuracy\": accuracy})\n            if i % 100 == 0:\n                logger.info(f\"Epoch {epoch}, iter {i}, loss: {loss.item()}\")\n\n        epoch_loss = epoch_loss / len(train_dataloader)\n        epoch_accuracy = epoch_accuracy / len(train_set)\n\n        logger.info(f\"Epoch {epoch}, loss: {epoch_loss}, accuracy: {epoch_accuracy}\")\n        wandb.log({\"epoch_train_loss\": epoch_loss, \"epoch_train_accuracy\": epoch_accuracy})\n\n        # Validation loop\n        model.eval()\n        val_loss, val_correct = 0.0, 0\n\n        with torch.no_grad():\n            for img, target in val_dataloader:\n                img, target = img.to(DEVICE), target.to(DEVICE)\n                y_pred = model(img)\n                loss = loss_fn(y_pred, target)\n                val_loss += loss.item()\n                val_correct += (y_pred.argmax(dim=1) == target).float().sum().item()\n\n        val_loss /= len(val_dataloader)\n        val_accuracy = val_correct / len(validation_set)\n        statistics[\"val_loss\"].append(val_loss)\n        statistics[\"val_accuracy\"].append(val_accuracy)\n\n        wandb.log({\"val_loss\": val_loss, \"val_accuracy\": val_accuracy})\n        logger.info(f\"Validation loss: {val_loss}, accuracy: {val_accuracy}\")\n\n    logger.info(\"Training complete\")\n    model_path = f\"{PROJECT_ROOT}/models/model.pth\"\n    torch.save(model.state_dict(), model_path)\n    onnx_model_path = f\"{PROJECT_ROOT}/models/model.onnx\"\n    torch.onnx.export(\n        model,\n        (img[0].unsqueeze(0),),\n        onnx_model_path,\n        input_names=[\"input\"],  # the model's input names\n        output_names=[\"output\"],\n    )\n    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n    axs[0].plot(statistics[\"train_loss\"])\n    axs[0].set_title(\"Train loss\")\n    axs[1].plot(statistics[\"train_accuracy\"])\n    axs[1].set_title(\"Train accuracy\")\n    fig_path = f\"{PROJECT_ROOT}/reports/figures/training_statistics.png\"\n    fig.savefig(fig_path)\n    run.log({\"training_statistics_via_matplotlib\": wandb.Image(fig_path)})\n\n    # Log model to wandb\n    artifact = wandb.Artifact(\n        name=\"leaf_classifier\",\n        type=\"model\",\n        description=\"A model trained to classify healthy and diseased plant leaves\",\n        metadata={\"accuracy\": statistics[\"train_accuracy\"][-1], \"loss\": statistics[\"train_loss\"][-1]},\n    )\n    artifact.add_file(model_path)\n    artifact.add_file(onnx_model_path)\n    run.log_artifact(artifact)\n\n    run.finish()\n</code></pre>"},{"location":"#plant_leaves.utils","title":"plant_leaves.utils","text":""},{"location":"#plant_leaves.utils.show_image_and_target","title":"show_image_and_target","text":"<pre><code>show_image_and_target(images, targets, show=True, grid_size=(5, 5))\n</code></pre> <p>Display a grid of images with their corresponding targets.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list or ndarray</code> <p>A list or array of images to display.</p> required <code>targets</code> <code>list or ndarray</code> <p>A list or array of targets corresponding to the images.</p> required <code>show</code> <code>bool</code> <p>Whether to display the plot interactively. Defaults to True.</p> <code>True</code> <code>grid_size</code> <code>tuple</code> <p>The dimensions of the grid (rows, cols). Defaults to (5, 5).</p> <code>(5, 5)</code> Source code in <code>plant_leaves/utils.py</code> <pre><code>def show_image_and_target(images, targets, show=True, grid_size=(5, 5)):\n    \"\"\"\n    Display a grid of images with their corresponding targets.\n\n    Args:\n        images (list or np.ndarray): A list or array of images to display.\n        targets (list or np.ndarray): A list or array of targets corresponding to the images.\n        show (bool): Whether to display the plot interactively. Defaults to True.\n        grid_size (tuple): The dimensions of the grid (rows, cols). Defaults to (5, 5).\n    \"\"\"\n    num_images = len(images)\n    rows, cols = grid_size\n\n    if num_images &lt; rows * cols:\n        print(f\"Warning: Not enough images to fill the grid ({rows}x{cols}).\")\n        rows = int(np.ceil(num_images / cols))\n\n    fig, axes = plt.subplots(rows, cols, figsize=(cols * 2, rows * 2))\n    axes = axes.flatten()\n\n    for i, ax in enumerate(axes):\n        if i &lt; num_images:\n            ax.imshow(images[i].to(torch.uint8), cmap=\"gray\")\n            ax.set_title(f\"Target: {targets[i]}\")\n        ax.axis(\"off\")\n\n    if show:\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"#plant_leaves.visualize","title":"plant_leaves.visualize","text":""},{"location":"#frontend.app","title":"frontend.app","text":""},{"location":"#frontend.app.image_to_bytes","title":"image_to_bytes","text":"<pre><code>image_to_bytes(image: Image, format: str = 'JPEG') -&gt; bytes\n</code></pre> <p>Converts a PIL Image to bytes.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The image to convert.</p> required <code>format</code> <code>str</code> <p>The format to save the image in (e.g., \"JPEG\").</p> <code>'JPEG'</code> <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The byte representation of the image.</p> Source code in <code>frontend/app.py</code> <pre><code>def image_to_bytes(image: Image.Image, format: str = \"JPEG\") -&gt; bytes:\n    \"\"\"\n    Converts a PIL Image to bytes.\n\n    Args:\n        image (Image.Image): The image to convert.\n        format (str): The format to save the image in (e.g., \"JPEG\").\n\n    Returns:\n        bytes: The byte representation of the image.\n    \"\"\"\n    byte_stream = io.BytesIO()\n    image.save(byte_stream, format=format)\n    byte_stream.seek(0)  # Move to the beginning of the byte stream\n    return byte_stream.read()\n</code></pre>"}]}